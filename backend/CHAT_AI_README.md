# Chat AI v·ªõi GGUF Models

H·ªá th·ªëng Chat AI s·ª≠ d·ª•ng m√¥ h√¨nh GGUF Qwen2.5-7B-Instruct, ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a cho RTX 3060 6GB.

## T√≠nh nƒÉng

- ü§ñ Chat AI th√¥ng minh v·ªõi m√¥ h√¨nh Qwen2.5-7B
- üöÄ H·ªó tr·ª£ GPU acceleration (CUDA)
- üíæ Qu·∫£n l√Ω b·ªô nh·ªõ th√¥ng minh cho RTX 3060 6GB
- üì° Streaming response
- üó£Ô∏è H·ªó tr·ª£ ti·∫øng Vi·ªát
- üîÑ API RESTful ho√†n ch·ªânh

## C·∫•u h√¨nh h·ªá th·ªëng

### Y√™u c·∫ßu

- **CPU**: Intel 12700H (8 threads ƒë∆∞·ª£c s·ª≠ d·ª•ng)
- **GPU**: RTX 3060 6GB VRAM
- **RAM**: 32GB (khuy·∫øn ngh·ªã t·ªëi thi·ªÉu 16GB)
- **Storage**: ~10GB ƒë·ªÉ l∆∞u models

### C·∫•u h√¨nh m√¥ h√¨nh ƒë∆∞·ª£c t·ªëi ∆∞u

```python
ModelConfig(
    model_name="gaianet/Qwen2.5-7B-Instruct-GGUF",
    model_file="qwen2.5-7b-instruct-q4_k_m.gguf",  # 4.68GB
    context_length=4096,      # Ph√π h·ª£p v·ªõi 6GB VRAM
    max_tokens=512,
    n_gpu_layers=25,          # S·ªë l·ªõp ch·∫°y tr√™n GPU
    n_threads=8,              # Cho 12700H
    temperature=0.7
)
```

## C√†i ƒë·∫∑t

### 1. C√†i ƒë·∫∑t dependencies

#### S·ª≠ d·ª•ng Poetry (khuy·∫øn ngh·ªã):

```bash
# Ch·∫°y script t·ª± ƒë·ªông
cd backend
install_chat_dependencies.bat

# Ho·∫∑c c√†i ƒë·∫∑t th·ªß c√¥ng:
poetry install
poetry add llama-cpp-python --extras="cuda"
poetry add huggingface_hub pydantic-settings
```

#### S·ª≠ d·ª•ng pip:

```bash
pip install -r requirements.txt
pip install llama-cpp-python huggingface_hub pydantic-settings
```

### 2. Ki·ªÉm tra c√†i ƒë·∫∑t

```bash
# Test ƒë∆°n gi·∫£n
poetry run python simple_chat_test.py

# Test ƒë·∫ßy ƒë·ªß
poetry run python test_chat_ai.py
```

## S·ª≠ d·ª•ng

### 1. Test Chat AI ƒë·ªôc l·∫≠p

```python
from app.core.ai_models import ChatAI, ModelConfig

# T·∫°o c·∫•u h√¨nh
config = ModelConfig(
    model_file="qwen2.5-7b-instruct-q4_k_m.gguf",
    n_gpu_layers=25,
    temperature=0.7
)

# Kh·ªüi t·∫°o chat AI
chat_ai = ChatAI(config)
chat_ai.load_model()

# Chat
response = chat_ai.chat("Xin ch√†o! B·∫°n c√≥ th·ªÉ gi√∫p t√¥i kh√¥ng?")
print(response)
```

### 2. S·ª≠ d·ª•ng API

#### Kh·ªüi ƒë·ªông server:

```bash
poetry run uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

#### API Endpoints:

**Chat th∆∞·ªùng:**

```bash
curl -X POST "http://localhost:8000/api/v1/ai/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Xin ch√†o!",
    "system_prompt": "B·∫°n l√† tr·ª£ l√Ω AI th√¥ng minh",
    "reset_history": false
  }'
```

**Stream chat:**

```bash
curl -X POST "http://localhost:8000/api/v1/ai/chat/stream" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "K·ªÉ cho t√¥i m·ªôt c√¢u chuy·ªán ng·∫Øn"
  }'
```

**Qu·∫£n l√Ω model:**

```bash
# Load model
curl -X POST "http://localhost:8000/api/v1/ai/model/load"

# Th√¥ng tin model
curl -X GET "http://localhost:8000/api/v1/ai/model/info"

# Unload model
curl -X POST "http://localhost:8000/api/v1/ai/model/unload"
```

## T·ªëi ∆∞u h√≥a cho RTX 3060 6GB

### C·∫•u h√¨nh VRAM th·∫•p:

```python
ModelConfig(
    model_file="qwen2.5-7b-instruct-q4_0.gguf",  # 4.43GB
    context_length=2048,     # Gi·∫£m context
    n_gpu_layers=20,         # √çt layer h∆°n tr√™n GPU
    max_tokens=256
)
```

### C·∫•u h√¨nh VRAM cao:

```python
ModelConfig(
    model_file="qwen2.5-7b-instruct-q5_k_m.gguf",  # 5.44GB
    context_length=4096,
    n_gpu_layers=30,
    max_tokens=512
)
```

## C√°c file quan tr·ªçng

### Core Components:

- `app/core/ai_models.py`: L·ªõp ChatAI ch√≠nh
- `app/api/v1/chat_ai.py`: API endpoints
- `test_chat_ai.py`: Test suite ƒë·∫ßy ƒë·ªß
- `simple_chat_test.py`: Test nhanh

### Configuration:

- `pyproject.toml`: Dependencies
- `install_chat_dependencies.bat`: Script c√†i ƒë·∫∑t

## Troubleshooting

### L·ªói CUDA:

```bash
# Ki·ªÉm tra CUDA
poetry run python -c "import torch; print(torch.cuda.is_available())"

# C√†i l·∫°i llama-cpp-python v·ªõi CUDA
set CMAKE_ARGS=-DLLAMA_CUBLAS=on
poetry run pip install llama-cpp-python --force-reinstall --no-cache-dir
```

### Thi·∫øu VRAM:

- Gi·∫£m `n_gpu_layers`
- Gi·∫£m `context_length`
- S·ª≠ d·ª•ng model nh·ªè h∆°n (Q4_0 thay v√¨ Q4_K_M)

### Model download ch·∫≠m:

```bash
# Download tr∆∞·ªõc b·∫±ng huggingface-cli
huggingface-cli download gaianet/Qwen2.5-7B-Instruct-GGUF qwen2.5-7b-instruct-q4_k_m.gguf
```

## Performance Benchmarks

Tr√™n RTX 3060 6GB + 12700H:

- **Load time**: ~30-60 gi√¢y
- **Response speed**: ~15-25 tokens/gi√¢y
- **VRAM usage**: ~5.5GB (Q4_K_M)
- **Context length**: 4096 tokens

## API Documentation

Truy c·∫≠p http://localhost:8000/docs ƒë·ªÉ xem Swagger UI v·ªõi t√†i li·ªáu API ƒë·∫ßy ƒë·ªß.

## T√≠ch h·ª£p

ƒê·ªÉ t√≠ch h·ª£p v√†o ·ª©ng d·ª•ng ch√≠nh:

1. Chat AI ƒë√£ ƒë∆∞·ª£c th√™m v√†o `main.py`
2. API endpoints c√≥ prefix `/api/v1/ai/`
3. C√≥ th·ªÉ s·ª≠ d·ª•ng c√πng v·ªõi TTS v√† RAG
4. H·ªó tr·ª£ streaming cho real-time chat

## Ph√°t tri·ªÉn ti·∫øp

- [ ] Fine-tuning cho domain c·ª• th·ªÉ
- [ ] T√≠ch h·ª£p v·ªõi RAG system
- [ ] Multi-model support
- [ ] Voice integration
- [ ] Chat history persistence
