# PersonaRAG LLM Integration

TÃ­ch há»£p mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n Qwen2.5-7B-Instruct vá»›i kháº£ nÄƒng fine-tuning LoRA cho PersonaRAG.

## ğŸ¯ TÃ­nh nÄƒng chÃ­nh

- **Qwen2.5-7B-Instruct**: MÃ´ hÃ¬nh chat AI tiÃªn tiáº¿n vá»›i 7 tá»· tham sá»‘
- **Unsloth Optimization**: Tá»‘i Æ°u hÃ³a tá»‘c Ä‘á»™ vÃ  bá»™ nhá»›
- **4-bit Quantization**: Giáº£m sá»­ dá»¥ng VRAM xuá»‘ng cÃ²n ~4.5GB
- **LoRA Fine-tuning**: Kháº£ nÄƒng fine-tune vá»›i tÃ i nguyÃªn háº¡n cháº¿
- **Vietnamese Support**: Há»— trá»£ tiáº¿ng Viá»‡t tá»± nhiÃªn

## ğŸ–¥ï¸ YÃªu cáº§u há»‡ thá»‘ng

### Cáº¥u hÃ¬nh tá»‘i thiá»ƒu (RTX 3060 6GB)

- **GPU**: RTX 3060 vá»›i 6GB VRAM
- **CPU**: i7-12700H hoáº·c tÆ°Æ¡ng Ä‘Æ°Æ¡ng
- **RAM**: 32GB há»‡ thá»‘ng
- **Storage**: 20GB dung lÆ°á»£ng trá»‘ng

### Pháº§n má»m

- Python 3.11
- CUDA 12.1+
- Poetry package manager

## ğŸš€ CÃ i Ä‘áº·t nhanh

### 1. Cháº¡y script setup

```bash
cd backend/scripts
setup_llm.bat
```

### 2. CÃ i Ä‘áº·t thá»§ cÃ´ng

```bash
# CÃ i Ä‘áº·t dependencies cÆ¡ báº£n
poetry install

# ThÃªm dependencies cho LLM
poetry add peft bitsandbytes psutil
```

## ğŸ§ª Kiá»ƒm tra vÃ  sá»­ dá»¥ng

### 1. Test cÆ¡ báº£n

```bash
poetry run python test_qwen_chat.py
```

### 2. Chat tÆ°Æ¡ng tÃ¡c

```bash
poetry run python test_qwen_chat.py --interactive
```

### 3. Test fine-tuning preparation

```bash
poetry run python test_qwen_chat.py --test-finetuning
```

## ğŸ“‹ Sá»­ dá»¥ng trong code

### Load model vÃ  chat cÆ¡ báº£n

```python
from app.core.ai_models import ai_models_manager

# Load model
ai_models_manager.load_chat_model()

# Chat
response = ai_models_manager.generate_chat_response(
    "Xin chÃ o! Báº¡n cÃ³ khá»e khÃ´ng?"
)
print(response)
```

### Chuáº©n bá»‹ fine-tuning

```python
# Prepare for LoRA fine-tuning
ai_models_manager.prepare_chat_for_finetuning(
    r=16,  # LoRA rank
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)
```

### Cáº¥u hÃ¬nh persona khÃ¡c nhau

```python
# Set system prompt cho persona
ai_models_manager.set_chat_system_prompt(
    "Báº¡n lÃ  má»™t giÃ¡o viÃªn kiÃªn nháº«n vÃ  nhiá»‡t tÃ¬nh..."
)
```

## âš™ï¸ Cáº¥u hÃ¬nh cho RTX 3060

### Memory Optimization

```python
# Cáº¥u hÃ¬nh tá»‘i Æ°u cho 6GB VRAM
config = {
    "max_seq_length": 2048,      # Giáº£m Ä‘á»™ dÃ i sequence
    "load_in_4bit": True,        # Báº¯t buá»™c 4-bit quantization
    "batch_size": 1,             # Batch size nhá»
    "gradient_accumulation": 8,   # BÃ¹ trá»« batch size nhá»
}
```

### LoRA Parameters

```python
# Cáº¥u hÃ¬nh LoRA phÃ¹ há»£p vá»›i 6GB VRAM
lora_config = {
    "r": 16,                     # Rank tháº¥p hÆ¡n
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj"]
}
```

## ğŸ”§ Troubleshooting

### Out of Memory (OOM) Errors

```bash
# Giáº£m sequence length
max_seq_length = 1024

# Giáº£m batch size
per_device_batch_size = 1

# TÄƒng gradient accumulation
gradient_accumulation_steps = 16
```

### Slow Generation

```bash
# Sá»­ dá»¥ng caching
use_cache = True

# Giáº£m sá»‘ tokens generate
max_new_tokens = 256

# Táº¯t sampling Ä‘á»ƒ nhanh hÆ¡n
do_sample = False
```

### Model Loading Issues

```bash
# XÃ³a cache náº¿u cÃ³ lá»—i
rm -rf ~/.cache/huggingface/

# CÃ i Ä‘áº·t láº¡i unsloth
poetry remove unsloth
poetry add git+https://github.com/unslothai/unsloth.git
```

## ğŸ“Š Performance Benchmarks

### RTX 3060 6GB Performance

- **Loading time**: ~2-3 phÃºt
- **Memory usage**: ~4.5GB VRAM
- **Generation speed**: ~15-20 tokens/giÃ¢y
- **Fine-tuning speed**: ~0.5 steps/giÃ¢y

### Memory Usage Breakdown

```
Model weights (4-bit): ~3.5GB
Activation memory: ~0.8GB
LoRA adapters: ~0.1GB
Buffer: ~0.1GB
Total: ~4.5GB / 6GB
```

## ğŸ¨ Persona System Prompts

### Friendly Assistant

```python
prompt = "Báº¡n lÃ  má»™t ngÆ°á»i báº¡n thÃ¢n thiá»‡n vÃ  vui váº». HÃ£y tráº£ lá»i vá»›i giá»ng Ä‘iá»‡u thÃ¢n máº­t vÃ  áº¥m Ã¡p."
```

### Professional Advisor

```python
prompt = "Báº¡n lÃ  má»™t chuyÃªn gia tÆ° váº¥n chuyÃªn nghiá»‡p. HÃ£y tráº£ lá»i má»™t cÃ¡ch chÃ­nh xÃ¡c vÃ  cÃ³ cÄƒn cá»©."
```

### Creative Writer

```python
prompt = "Báº¡n lÃ  má»™t nghá»‡ sÄ© sÃ¡ng táº¡o. HÃ£y tráº£ lá»i vá»›i Ã³c tÆ°á»Ÿng tÆ°á»£ng phong phÃº vÃ  Ä‘á»™c Ä‘Ã¡o."
```

## ğŸ”„ Integration vá»›i F5-TTS

```python
# Workflow: Text -> LLM -> TTS -> Audio
user_input = "Ká»ƒ cho tÃ´i má»™t cÃ¢u chuyá»‡n ngáº¯n"

# 1. Generate text response
text_response = ai_models_manager.generate_chat_response(user_input)

# 2. Convert to speech with F5-TTS (TODO)
# audio = tts_service.synthesize(text_response, voice_profile="gia_cat_luong")
```

## ğŸ“ Logs vÃ  Monitoring

### Check GPU Memory

```python
status = ai_models_manager.get_models_status()
print(f"GPU Memory: {status['memory_usage']['chat']['allocated']:.2f} GB")
```

### Training Logs

```bash
# Sá»­ dá»¥ng wandb cho monitoring (optional)
poetry add wandb
wandb login
```

## ğŸ› ï¸ Development Notes

### Model Architecture

- **Base Model**: Qwen2.5-7B-Instruct
- **Quantization**: BitsAndBytesConfig 4-bit NF4
- **Optimization**: Unsloth + Flash Attention
- **Fine-tuning**: LoRA (Low-Rank Adaptation)

### File Structure

```
backend/
â”œâ”€â”€ app/core/
â”‚   â”œâ”€â”€ llm_chat.py         # Main chat model class
â”‚   â”œâ”€â”€ llm_config.py       # Configuration settings
â”‚   â””â”€â”€ ai_models.py        # Models manager
â”œâ”€â”€ test_qwen_chat.py       # Test script
â””â”€â”€ scripts/
    â””â”€â”€ setup_llm.bat       # Setup script
```

## ğŸ”— TÃ i liá»‡u tham kháº£o

- [Unsloth Documentation](https://github.com/unslothai/unsloth)
- [Qwen2.5 Model Card](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)
- [PEFT Documentation](https://huggingface.co/docs/peft)
- [BitsAndBytesConfig](https://huggingface.co/docs/transformers/main_classes/quantization)

## ğŸ“ Há»— trá»£

Náº¿u gáº·p váº¥n Ä‘á»:

1. Kiá»ƒm tra system requirements
2. Cháº¡y `test_qwen_chat.py` Ä‘á»ƒ debug
3. Kiá»ƒm tra GPU memory vá»›i `nvidia-smi`
4. Xem logs trong console output
